# wait-for-ollama.sh
#!/bin/bash
echo "ðŸ–– Waiting for Ollama at $OLLAMA_URL..."

until curl -s "$OLLAMA_URL/api/tags" > /dev/null; do
  sleep 1
done

echo "âœ… Ollama is ready."

# Check if DEFAULT_MODEL is available, if not, pull it
echo "ðŸ” Checking if model '$DEFAULT_MODEL' is available..."
if curl -s "$OLLAMA_URL/api/tags" | grep -q "\"name\":\"$DEFAULT_MODEL\""; then
  echo "âœ… Model '$DEFAULT_MODEL' is already available."
else
  echo "ðŸ“¥ Model '$DEFAULT_MODEL' not found. Pulling it now..."
  curl -X POST "$OLLAMA_URL/api/pull" -H "Content-Type: application/json" -d "{\"name\":\"$DEFAULT_MODEL\"}"
  echo ""
  echo "âœ… Model '$DEFAULT_MODEL' has been pulled successfully."
fi

exec "$@"

# Warm up ollama with the default model
echo "ðŸ¤– Warming up Ollama with $DEFAULT_MODEL..."
curl -s "$OLLAMA_URL/api/generate" -X POST -H "Content-Type: application/json" -d "{\"model\":\"$DEFAULT_MODEL\", \"prompt\":\"Hello, Ollama!\"}" > /dev/null
echo "âœ… Ollama is warmed up."

# Warm up our CLI by making a dummy request
echo "ðŸ¤– Warming up our CLI..."
python3 lcars/cli.py -q "Who is Data?" > /dev/null
echo "âœ… CLI is warmed up."