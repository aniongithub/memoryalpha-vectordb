#! /usr/bin/env python3


import os
from lxml import etree
import mwparserfromhell
import re

import sys
import pysqlite3
sys.modules["sqlite3"] = pysqlite3

import chromadb
from chromadb.config import Settings

SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
# Adjust paths relative to the script directory
INPUT_XML = os.path.join(SCRIPT_DIR, "../data/enmemoryalpha_pages_current.xml") # Path to the db .xml file
OUTPUT_DIR = os.path.join(SCRIPT_DIR, "../data")
os.makedirs(OUTPUT_DIR, exist_ok=True)

CHROMA_DB_DIR = os.path.join(SCRIPT_DIR, "../data/enmemoryalpha_db")
MAX_PAGES = os.getenv("MAX_PAGES", -1) # Get the max pages to process from the environment

def extract_and_store_chromadb(xml_path, chroma_db_dir, max_pages = -1):
    # Set up ChromaDB persistent client and collection
    client = chromadb.PersistentClient(path=chroma_db_dir, settings=Settings(allow_reset=True))
    collection = client.get_or_create_collection("memoryalpha")

    context = etree.iterparse(xml_path, tag="{http://www.mediawiki.org/xml/export-0.11/}page", events=("end",), recover=True)
    count = 0
    batch_docs = []
    batch_metadatas = []
    batch_ids = []
    BATCH_SIZE = 100

    for _, elem in context:
        title_elem = elem.find("{http://www.mediawiki.org/xml/export-0.11/}title")
        title = title_elem.text if title_elem is not None else ""
        text_elem = elem.find(".//{http://www.mediawiki.org/xml/export-0.11/}text")
        raw_text = text_elem.text if text_elem is not None else ""

        if not raw_text or raw_text.strip().lower().startswith("#redirect"):
            elem.clear()
            continue
        if (title.lower().startswith(("file:", "user:", "talk:", "user talk:", 
                                     "memory alpha:", "memory alpha talk:", 
                                     "mediawiki:", "mediawiki talk:", "template:", 
                                     "template talk:", "category:", "category talk:"))):
            elem.clear()
            continue
        try:
            clean_text = mwparserfromhell.parse(raw_text).strip_code().strip()
        except Exception as e:
            print(f"‚ö†Ô∏è  Error parsing {title}: {e}")
            elem.clear()
            continue
        if len(clean_text) < 500:
            elem.clear()
            continue

        # Use a safe unique id for ChromaDB (sanitized title + count for uniqueness)
        base_id = re.sub(r'[^a-zA-Z0-9_\-]', '_', title)[:48] or "untitled"
        safe_id = f"{base_id}_{count}"
        batch_ids.append(safe_id)
        batch_docs.append(clean_text)
        batch_metadatas.append({"title": title})
        count += 1
        elem.clear()

        if len(batch_docs) >= BATCH_SIZE:
            collection.add(documents=batch_docs, metadatas=batch_metadatas, ids=batch_ids)
            batch_docs, batch_metadatas, batch_ids = [], [], []
            print(f"üìÑ Processed {count} pages...")

        # Check if we've reached the max pages limit, -1 means no limit
        if max_pages != -1 and count >= max_pages:
            break

    # Add any remaining docs
    if batch_docs:
        collection.add(documents=batch_docs, metadatas=batch_metadatas, ids=batch_ids)

    print(f"‚úÖ Extracted and stored {count} pages in ChromaDB at: {chroma_db_dir}")

# Run it
extract_and_store_chromadb(INPUT_XML, CHROMA_DB_DIR, MAX_PAGES)
